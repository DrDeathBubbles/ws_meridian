{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a630d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "\n",
    "from meridian import constants\n",
    "from meridian.data import load\n",
    "from meridian.data import test_utils\n",
    "from meridian.model import model\n",
    "from meridian.model import spec\n",
    "from meridian.model import prior_distribution\n",
    "from meridian.analysis import optimizer\n",
    "from meridian.analysis import analyzer\n",
    "from meridian.analysis import visualizer\n",
    "from meridian.analysis import summarizer\n",
    "from meridian.analysis import formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab516f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kk/x9w58sq1639gdzg86gw_hfg00000gp/T/ipykernel_97270/1764015688.py:1: DtypeWarning: Columns (25,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  marketing_spend= pd.read_csv('/Users/aaronmeagher/Work/google_meridian/google/ws_attempt/data/raw_data/advertising_raw.csv')\n"
     ]
    }
   ],
   "source": [
    "marketing_spend= pd.read_csv('/Users/aaronmeagher/Work/google_meridian/google/ws_attempt/data/raw_data/advertising_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c77df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_marketing_spend(df):\n",
    "    \"\"\"\n",
    "    Process marketing spend dataframe to create platform-specific spend columns\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe with marketing_platform_name and marketing_spend columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Processed dataframe with platform-specific spend columns\n",
    "    \"\"\"\n",
    "    # List of platforms\n",
    "    platforms = ['reddit', 'linkedin', 'facebook', 'google', 'bing', \n",
    "                'tiktok', 'twitter', 'instagram']\n",
    "    \n",
    "    # Create new spend columns initialized to 0\n",
    "    for platform in platforms:\n",
    "        df[f'{platform}_spend'] = 0\n",
    "        \n",
    "    # Iterate through each row and assign spend to appropriate column\n",
    "    for idx, row in df.iterrows():\n",
    "        platform = row['marketing_platform_name'].lower()\n",
    "        if platform in platforms:\n",
    "            df.at[idx, f'{platform}_spend'] = row['campaign_spend_eur']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_marketing_impressions(df):\n",
    "    \"\"\"\n",
    "    Process marketing spend dataframe to create platform-specific spend columns\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe with marketing_platform_name and marketing_spend columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Processed dataframe with platform-specific spend columns\n",
    "    \"\"\"\n",
    "    # List of platforms\n",
    "    platforms = ['reddit', 'linkedin', 'facebook', 'google', 'bing', \n",
    "                'tiktok', 'twitter', 'instagram']\n",
    "    \n",
    "    # Create new spend columns initialized to 0\n",
    "    for platform in platforms:\n",
    "        df[f'{platform}_impressions'] = 0\n",
    "        \n",
    "    # Iterate through each row and assign spend to appropriate column\n",
    "    for idx, row in df.iterrows():\n",
    "        platform = row['marketing_platform_name'].lower()\n",
    "        if platform in platforms:\n",
    "            df.at[idx, f'{platform}_spend'] = row['impressions']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# marketing_spend_processed = process_marketing_spend(marketing_spend)\n",
    "\n",
    "# To verify the results:\n",
    "# print(marketing_spend_processed.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f987a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kk/x9w58sq1639gdzg86gw_hfg00000gp/T/ipykernel_97270/623930320.py:23: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '147.37319' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, f'{platform}_spend'] = row['campaign_spend_eur']\n",
      "/var/folders/kk/x9w58sq1639gdzg86gw_hfg00000gp/T/ipykernel_97270/623930320.py:23: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '2231.85' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, f'{platform}_spend'] = row['campaign_spend_eur']\n",
      "/var/folders/kk/x9w58sq1639gdzg86gw_hfg00000gp/T/ipykernel_97270/623930320.py:23: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.02' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[idx, f'{platform}_spend'] = row['campaign_spend_eur']\n"
     ]
    }
   ],
   "source": [
    "processed_marketing_spend = process_marketing_spend(marketing_spend)\n",
    "processed_marketing_spend = process_marketing_impressions(processed_marketing_spend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "504d3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_web_summit = processed_marketing_spend[processed_marketing_spend['event_name'].str.contains('Web Summit 2023')]\n",
    "#df_web_summit.to_csv('./ws_attempt/data/raw_data/marketing_spend_processed_new_approach_web_summit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1aa57aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kk/x9w58sq1639gdzg86gw_hfg00000gp/T/ipykernel_97270/3103639258.py:3: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_web_summit_grouped= df_web_summit_grouped.interpolate(method='linear')\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = df_web_summit.select_dtypes(include=['int64', 'float64']).columns\n",
    "df_web_summit_grouped = df_web_summit.groupby('date_id')[numeric_columns].sum().reset_index()\n",
    "df_web_summit_grouped= df_web_summit_grouped.interpolate(method='linear')\n",
    "df_web_summit_grouped.to_csv('/Users/aaronmeagher/Work/google_meridian/google/ws_attempt/data/raw_data/marketing_spend_processed_new_approach_web_summit_grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "402335e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types before conversion:\n",
      "date_id       datetime64[ns]\n",
      "event_date    datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Data types after conversion:\n",
      "date_id       datetime64[ns]\n",
      "event_date    datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Rows with invalid dates:\n",
      "0\n",
      "0\n",
      "\n",
      "Number of unique geos: 240\n",
      "Sample geos: ['---' 'ABW' 'AFG' 'AGO' 'AIA']\n",
      "\n",
      "Date range: 2022-10-14 00:00:00 to 2024-06-30 00:00:00\n",
      "\n",
      "Most common day differences:\n",
      "days_diff\n",
      "0.0     121744\n",
      "1.0      55034\n",
      "2.0       2213\n",
      "3.0       1038\n",
      "4.0        604\n",
      "5.0        285\n",
      "6.0        187\n",
      "10.0       157\n",
      "7.0        145\n",
      "14.0       140\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Created 90 dates with frequency W\n",
      "\n",
      "Aggregating data by geo and date...\n",
      "Reindexing to include all geo-date combinations...\n",
      "Filling missing values with interpolation...\n",
      "\n",
      "Data has been regularized and saved to '/Users/aaronmeagher/Work/google_meridian/google/ws_attempt/data/raw_data/regularized_web_summit_data.csv'\n",
      "Shape of original data: (182693, 53)\n",
      "Shape of regularized data: (90, 29)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add this code to your notebook\n",
    "\n",
    "# 1. First, let's check the data types\n",
    "print(\"Data types before conversion:\")\n",
    "print(df_web_summit[['date_id', 'event_date']].dtypes)\n",
    "\n",
    "# 2. Convert date columns to datetime\n",
    "df_web_summit['date_id'] = pd.to_datetime(df_web_summit['date_id'])\n",
    "df_web_summit['event_date'] = pd.to_datetime(df_web_summit['event_date'])\n",
    "\n",
    "# 3. Verify conversion\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(df_web_summit[['date_id', 'event_date']].dtypes)\n",
    "\n",
    "# 4. Check for any rows where conversion failed (would be NaT)\n",
    "print(\"\\nRows with invalid dates:\")\n",
    "print(df_web_summit[df_web_summit['date_id'].isna()].shape[0])\n",
    "print(df_web_summit[df_web_summit['event_date'].isna()].shape[0])\n",
    "\n",
    "# 5. Identify your geo column (using reach_country_id)\n",
    "geo_column = 'reach_country_id'\n",
    "\n",
    "# 6. Check unique values in geo column\n",
    "unique_geos = df_web_summit[geo_column].unique()\n",
    "print(f\"\\nNumber of unique geos: {len(unique_geos)}\")\n",
    "print(f\"Sample geos: {unique_geos[:5]}\")\n",
    "\n",
    "# 7. Find min and max dates\n",
    "min_date = df_web_summit['date_id'].min()\n",
    "max_date = df_web_summit['date_id'].max()\n",
    "print(f\"\\nDate range: {min_date} to {max_date}\")\n",
    "\n",
    "# 8. Sort by geo and date\n",
    "df_web_summit = df_web_summit.sort_values([geo_column, 'date_id'])\n",
    "\n",
    "# 9. Calculate days between dates within each geo group\n",
    "# Use a safer approach that handles the first row in each group\n",
    "df_web_summit['days_diff'] = df_web_summit.groupby(geo_column)['date_id'].diff().dt.days\n",
    "\n",
    "# 10. Check the distribution of day differences\n",
    "days_diff_counts = df_web_summit['days_diff'].value_counts().head(10)\n",
    "print(\"\\nMost common day differences:\")\n",
    "print(days_diff_counts)\n",
    "\n",
    "# 11. Choose a frequency based on the most common interval\n",
    "# For marketing data, weekly is often appropriate\n",
    "freq = 'W'  # Weekly\n",
    "\n",
    "# 12. Create a complete date range\n",
    "date_range = pd.date_range(start=min_date, end=max_date, freq=freq)\n",
    "print(f\"\\nCreated {len(date_range)} dates with frequency {freq}\")\n",
    "\n",
    "# 13. Create a MultiIndex for all geo-date combinations\n",
    "# Filter out any problematic geos (like '---' or empty strings)\n",
    "valid_geos = [geo for geo in unique_geos if geo not in ['---', '', None]]\n",
    "if len(valid_geos) == 0:\n",
    "    # If no valid geos, create a dummy one\n",
    "    valid_geos = ['default_geo']\n",
    "    df_web_summit[geo_column] = 'default_geo'\n",
    "\n",
    "complete_index = pd.MultiIndex.from_product(\n",
    "    [valid_geos, date_range],\n",
    "    names=[geo_column, 'date_id']\n",
    ")\n",
    "\n",
    "# 14. Prepare numeric columns for aggregation\n",
    "numeric_cols = df_web_summit.select_dtypes(include=['number']).columns.tolist()\n",
    "# Remove any columns you don't want to aggregate\n",
    "if 'Unnamed: 0' in numeric_cols:\n",
    "    numeric_cols.remove('Unnamed: 0')\n",
    "\n",
    "# 15. Aggregate data by geo and date\n",
    "print(\"\\nAggregating data by geo and date...\")\n",
    "aggregated_df = df_web_summit.groupby([geo_column, 'date_id'])[numeric_cols].sum().reset_index()\n",
    "\n",
    "# 16. Reindex to include all geo-date combinations\n",
    "print(\"Reindexing to include all geo-date combinations...\")\n",
    "complete_df = aggregated_df.set_index([geo_column, 'date_id']).reindex(complete_index)\n",
    "\n",
    "# 17. Fill missing values with interpolation within each geo\n",
    "print(\"Filling missing values with interpolation...\")\n",
    "for geo in valid_geos:\n",
    "    geo_mask = complete_df.index.get_level_values(geo_column) == geo\n",
    "    complete_df.loc[geo_mask] = complete_df.loc[geo_mask].interpolate(method='linear')\n",
    "\n",
    "# 18. Fill any remaining NaNs with 0\n",
    "complete_df = complete_df.fillna(0)\n",
    "\n",
    "# 19. Reset index to get geo and date back as columns\n",
    "regularized_df = complete_df.reset_index()\n",
    "\n",
    "# This is removing the geos and also summing the resulting data\n",
    "#regularized_df['reach_country_id'] = \"TOTAl\"\n",
    "regularized_df['geo'] = \"TOTAL\"\n",
    "regularized_df = regularized_df.groupby('date_id').sum(numeric_only=True).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# 20. Save to CSV\n",
    "output_path = '/Users/aaronmeagher/Work/google_meridian/google/ws_attempt/data/raw_data/regularized_web_summit_data.csv'\n",
    "regularized_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nData has been regularized and saved to '{output_path}'\")\n",
    "print(f\"Shape of original data: {df_web_summit.shape}\")\n",
    "print(f\"Shape of regularized data: {regularized_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f44a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_id\n",
       "2022-10-16    1\n",
       "2024-01-28    1\n",
       "2024-01-14    1\n",
       "2024-01-07    1\n",
       "2023-12-31    1\n",
       "2023-12-24    1\n",
       "2023-12-17    1\n",
       "2023-12-10    1\n",
       "2023-12-03    1\n",
       "2023-11-26    1\n",
       "2023-11-19    1\n",
       "2023-11-12    1\n",
       "2023-11-05    1\n",
       "2023-10-29    1\n",
       "2023-10-22    1\n",
       "2023-10-15    1\n",
       "2023-10-08    1\n",
       "2023-10-01    1\n",
       "2023-09-24    1\n",
       "2023-09-17    1\n",
       "2023-09-10    1\n",
       "2024-01-21    1\n",
       "2024-02-04    1\n",
       "2022-10-23    1\n",
       "2024-02-11    1\n",
       "2024-06-23    1\n",
       "2024-06-16    1\n",
       "2024-06-09    1\n",
       "2024-06-02    1\n",
       "2024-05-26    1\n",
       "2024-05-19    1\n",
       "2024-05-12    1\n",
       "2024-05-05    1\n",
       "2024-04-28    1\n",
       "2024-04-21    1\n",
       "2024-04-14    1\n",
       "2024-04-07    1\n",
       "2024-03-31    1\n",
       "2024-03-24    1\n",
       "2024-03-17    1\n",
       "2024-03-10    1\n",
       "2024-03-03    1\n",
       "2024-02-25    1\n",
       "2024-02-18    1\n",
       "2023-09-03    1\n",
       "2023-08-27    1\n",
       "2023-08-20    1\n",
       "2023-08-13    1\n",
       "2023-03-05    1\n",
       "2023-02-26    1\n",
       "2023-02-19    1\n",
       "2023-02-12    1\n",
       "2023-02-05    1\n",
       "2023-01-29    1\n",
       "2023-01-22    1\n",
       "2023-01-15    1\n",
       "2023-01-08    1\n",
       "2023-01-01    1\n",
       "2022-12-25    1\n",
       "2022-12-18    1\n",
       "2022-12-11    1\n",
       "2022-12-04    1\n",
       "2022-11-27    1\n",
       "2022-11-20    1\n",
       "2022-11-13    1\n",
       "2022-11-06    1\n",
       "2022-10-30    1\n",
       "2023-03-12    1\n",
       "2023-03-19    1\n",
       "2023-03-26    1\n",
       "2023-06-11    1\n",
       "2023-08-06    1\n",
       "2023-07-30    1\n",
       "2023-07-23    1\n",
       "2023-07-16    1\n",
       "2023-07-09    1\n",
       "2023-07-02    1\n",
       "2023-06-25    1\n",
       "2023-06-18    1\n",
       "2023-06-04    1\n",
       "2023-04-02    1\n",
       "2023-05-28    1\n",
       "2023-05-21    1\n",
       "2023-05-14    1\n",
       "2023-05-07    1\n",
       "2023-04-30    1\n",
       "2023-04-23    1\n",
       "2023-04-16    1\n",
       "2023-04-09    1\n",
       "2024-06-30    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularized_df['date_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb306443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>conference_year</th>\n",
       "      <th>event_currency_id</th>\n",
       "      <th>currency_id</th>\n",
       "      <th>impressions</th>\n",
       "      <th>marketing_platform_id</th>\n",
       "      <th>clicks</th>\n",
       "      <th>reach</th>\n",
       "      <th>campaign_spend_eur</th>\n",
       "      <th>days_to_event</th>\n",
       "      <th>campaign_spend</th>\n",
       "      <th>weeks_to_event</th>\n",
       "      <th>reddit_spend</th>\n",
       "      <th>linkedin_spend</th>\n",
       "      <th>facebook_spend</th>\n",
       "      <th>google_spend</th>\n",
       "      <th>bing_spend</th>\n",
       "      <th>tiktok_spend</th>\n",
       "      <th>twitter_spend</th>\n",
       "      <th>instagram_spend</th>\n",
       "      <th>reddit_impressions</th>\n",
       "      <th>linkedin_impressions</th>\n",
       "      <th>facebook_impressions</th>\n",
       "      <th>google_impressions</th>\n",
       "      <th>bing_impressions</th>\n",
       "      <th>tiktok_impressions</th>\n",
       "      <th>twitter_impressions</th>\n",
       "      <th>instagram_impressions</th>\n",
       "      <th>days_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-16</td>\n",
       "      <td>946764.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>3018606.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>85845.0</td>\n",
       "      <td>385186.0</td>\n",
       "      <td>5238.031181</td>\n",
       "      <td>183924.0</td>\n",
       "      <td>5238.031181</td>\n",
       "      <td>26208.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>232.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_id  conference_year  event_currency_id  currency_id  impressions  \\\n",
       "0 2022-10-16         946764.0              468.0        468.0    3018606.0   \n",
       "\n",
       "   marketing_platform_id   clicks     reach  campaign_spend_eur  \\\n",
       "0                  693.0  85845.0  385186.0         5238.031181   \n",
       "\n",
       "   days_to_event  campaign_spend  weeks_to_event  reddit_spend  \\\n",
       "0       183924.0     5238.031181         26208.0           0.0   \n",
       "\n",
       "   linkedin_spend  facebook_spend  google_spend  bing_spend  tiktok_spend  \\\n",
       "0             0.0             0.0           0.0         0.0           0.0   \n",
       "\n",
       "   twitter_spend  instagram_spend  reddit_impressions  linkedin_impressions  \\\n",
       "0            0.0              0.0                 0.0                   0.0   \n",
       "\n",
       "   facebook_impressions  google_impressions  bing_impressions  \\\n",
       "0                   0.0                 0.0               0.0   \n",
       "\n",
       "   tiktok_impressions  twitter_impressions  instagram_impressions  days_diff  \n",
       "0                 0.0                  0.0                    0.0      232.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularized_df[regularized_df['date_id'] == '2022-10-16'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e14f12cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "geos already set to ['national_geo'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     42\u001b[39m correct_media_spend_to_channel = {\n\u001b[32m     43\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfacebook_spend\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mFacebook\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     44\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlinkedin_spend\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mLinkedIn\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minstagram_spend\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mInstagram\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     51\u001b[39m }\n\u001b[32m     55\u001b[39m loader = load.CsvDataLoader(\n\u001b[32m     56\u001b[39m     csv_path=\u001b[33m\"\u001b[39m\u001b[33m/Users/aaronmeagher/Work/google_meridian/google/ws_attempt/data/raw_data/regularized_web_summit_data.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     57\u001b[39m     kpi_type=\u001b[33m'\u001b[39m\u001b[33mnon_revenue\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     media_spend_to_channel=correct_media_spend_to_channel,\n\u001b[32m     61\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m data = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m roi_mu = \u001b[32m1\u001b[39m \u001b[38;5;66;03m#0.2     # Mu for ROI prior for each media channel.\u001b[39;00m\n\u001b[32m     67\u001b[39m roi_sigma = \u001b[32m5\u001b[39m \u001b[38;5;66;03m#0.9  # Sigma for ROI prior for each media channel.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/meridian/data/load.py:1094\u001b[39m, in \u001b[36mCsvDataLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> input_data.InputData:\n\u001b[32m   1092\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Reads data from a CSV file and returns an `InputData` object.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_df_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/meridian/data/load.py:843\u001b[39m, in \u001b[36mDataFrameDataLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    839\u001b[39m   builder.with_population(\n\u001b[32m    840\u001b[39m       \u001b[38;5;28mself\u001b[39m.df, \u001b[38;5;28mself\u001b[39m.coord_to_columns.population, \u001b[38;5;28mself\u001b[39m.coord_to_columns.geo\n\u001b[32m    841\u001b[39m   )\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coord_to_columns.controls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m   \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_controls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcoord_to_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcoord_to_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcoord_to_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgeo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coord_to_columns.non_media_treatments \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    850\u001b[39m   builder.with_non_media_treatments(\n\u001b[32m    851\u001b[39m       \u001b[38;5;28mself\u001b[39m.df,\n\u001b[32m    852\u001b[39m       \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.coord_to_columns.non_media_treatments),\n\u001b[32m    853\u001b[39m       \u001b[38;5;28mself\u001b[39m.coord_to_columns.time,\n\u001b[32m    854\u001b[39m       \u001b[38;5;28mself\u001b[39m.coord_to_columns.geo,\n\u001b[32m    855\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/meridian/data/data_frame_input_data_builder.py:107\u001b[39m, in \u001b[36mDataFrameInputDataBuilder.with_controls\u001b[39m\u001b[34m(self, df, control_cols, time_col, geo_col)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m### Transform ###\u001b[39;00m\n\u001b[32m    106\u001b[39m data = controls_df.set_index([geo_col, time_col])[control_cols].stack()\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontrols\u001b[49m = (\n\u001b[32m    108\u001b[39m     data.rename(constants.CONTROLS)\n\u001b[32m    109\u001b[39m     .rename_axis(\n\u001b[32m    110\u001b[39m         [constants.GEO, constants.TIME, constants.CONTROL_VARIABLE]\n\u001b[32m    111\u001b[39m     )\n\u001b[32m    112\u001b[39m     .to_xarray()\n\u001b[32m    113\u001b[39m )\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/meridian/data/input_data_builder.py:200\u001b[39m, in \u001b[36mInputDataBuilder.controls\u001b[39m\u001b[34m(self, controls)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_set(\u001b[33m'\u001b[39m\u001b[33mControls\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m.controls)\n\u001b[32m    199\u001b[39m \u001b[38;5;28mself\u001b[39m._controls = \u001b[38;5;28mself\u001b[39m._normalize_coords(controls, constants.TIME)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgeos\u001b[49m = \u001b[38;5;28mself\u001b[39m.controls.coords[constants.GEO].values.tolist()\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.time_coords = \u001b[38;5;28mself\u001b[39m.controls.coords[constants.TIME].values.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/meridian/data/input_data_builder.py:137\u001b[39m, in \u001b[36mInputDataBuilder.geos\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    135\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mGeos must be unique.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.geos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m.geos) != \u001b[38;5;28mset\u001b[39m(value):\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mgeos already set to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.geos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    138\u001b[39m \u001b[38;5;28mself\u001b[39m._geos = value\n",
      "\u001b[31mValueError\u001b[39m: geos already set to ['national_geo']."
     ]
    }
   ],
   "source": [
    "coord_to_columns = load.CoordToColumns(\n",
    "    time='date_id',\n",
    "    geo='geo',\n",
    "    controls=[],\n",
    "    #population='population',\n",
    "    kpi='clicks',\n",
    "    #revenue_per_kpi='revenue_per_conversion',\n",
    "    media=[\n",
    "        'facebook_impressions',\n",
    "        'linkedin_impressions',\n",
    "        'google_impressions',\n",
    "        'bing_impressions',\n",
    "        'tiktok_impressions',\n",
    "        'twitter_impressions',\n",
    "        'reddit_impressions',\n",
    "        'instagram_impressions',\n",
    "    ],\n",
    "    media_spend=[\n",
    "        'facebook_spend',\n",
    "        'linkedin_spend',\n",
    "        'google_spend',\n",
    "        'bing_spend',\n",
    "        'tiktok_spend',\n",
    "        'twitter_spend',\n",
    "        'reddit_spend',\n",
    "        'instagram_spend',\n",
    "    ],\n",
    "    #organic_media=['Organic_channel0_impression'],\n",
    "    #non_media_treatments=['Promo'],\n",
    ")\n",
    "\n",
    "correct_media_to_channel = {\n",
    "    'facebook_impressions': 'Facebook',\n",
    "    'linkedin_impressions': 'LinkedIn',\n",
    "    'google_impressions': 'Google',\n",
    "    'bing_impressions': 'Bing',\n",
    "    'tiktok_impressions': 'TickTock',\n",
    "    'twitter_impressions': 'Twitter',\n",
    "    'reddit_impressions': 'Reddit',\n",
    "    'instagram_impressions': 'Instagram',\n",
    "}\n",
    "correct_media_spend_to_channel = {\n",
    "    'facebook_spend': 'Facebook',\n",
    "    'linkedin_spend': 'LinkedIn',\n",
    "    'google_spend': 'Google',\n",
    "    'bing_spend': 'Bing',\n",
    "    'tiktok_spend': 'TickTock',\n",
    "    'twitter_spend': 'Twitter',\n",
    "    'reddit_spend': 'Reddit',\n",
    "    'instagram_spend': 'Instagram',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "loader = load.CsvDataLoader(\n",
    "    csv_path=\"/Users/aaronmeagher/Work/google_meridian/google/ws_attempt/data/raw_data/regularized_web_summit_data.csv\",\n",
    "    kpi_type='non_revenue',\n",
    "    coord_to_columns=coord_to_columns,\n",
    "    media_to_channel=correct_media_to_channel,\n",
    "    media_spend_to_channel=correct_media_spend_to_channel,\n",
    ")\n",
    "\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "roi_mu = 1 #0.2     # Mu for ROI prior for each media channel.\n",
    "roi_sigma = 5 #0.9  # Sigma for ROI prior for each media channel.\n",
    "prior = prior_distribution.PriorDistribution(\n",
    "    roi_m=tfp.distributions.LogNormal(roi_mu, roi_sigma, name=constants.ROI_M)\n",
    ")\n",
    "model_spec = spec.ModelSpec(prior=prior, media_effects_dist='normal')\n",
    "\n",
    "mmm = model.Meridian(input_data=data, model_spec=model_spec)\n",
    "\n",
    "mmm.sample_prior(500)\n",
    "mmm.sample_posterior(n_chains=7, n_adapt=500, n_burnin=500, n_keep=2000, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35122def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize your data before feeding it to Meridian\n",
    "# For each numeric column that varies widely\n",
    "def normalise_column(df):\n",
    "    columns = [\n",
    "        'impressions',  'clicks',\n",
    "       'campaign_spend_eur', 'campaign_spend',\n",
    "        'reddit_spend', 'linkedin_spend', 'facebook_spend',\n",
    "       'google_spend', 'bing_spend', 'tiktok_spend', 'twitter_spend',\n",
    "       'instagram_spend', 'reddit_impressions', 'linkedin_impressions',\n",
    "       'facebook_impressions', 'google_impressions', 'bing_impressions',\n",
    "       'tiktok_impressions', 'twitter_impressions', 'instagram_impressions',\n",
    "       ]\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9db097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_df = pd.DataFrame(regularized_df)\n",
    "regularized_df.to_csv('/Users/aaronmeagher/Work/google_meridian/google/ws_attempt/data/raw_data/regularized_normalised_df_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0847a2f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Duplicate entries found in the 'time' column.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     42\u001b[39m correct_media_spend_to_channel = {\n\u001b[32m     43\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfacebook_spend\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mFacebook\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     44\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlinkedin_spend\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mLinkedIn\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minstagram_spend\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mInstagram\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     51\u001b[39m }\n\u001b[32m     55\u001b[39m loader = load.CsvDataLoader(\n\u001b[32m     56\u001b[39m     csv_path=\u001b[33m\"\u001b[39m\u001b[33m/Users/aaronmeagher/Work/google_meridian/google/ws_attempt/data/raw_data/regularized_normalised_df_2.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     57\u001b[39m     kpi_type=\u001b[33m'\u001b[39m\u001b[33mnon_revenue\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     media_spend_to_channel=correct_media_spend_to_channel,\n\u001b[32m     61\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m data = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m roi_mu = \u001b[32m1\u001b[39m \u001b[38;5;66;03m#0.2     # Mu for ROI prior for each media channel.\u001b[39;00m\n\u001b[32m     67\u001b[39m roi_sigma = \u001b[32m5\u001b[39m \u001b[38;5;66;03m#0.9  # Sigma for ROI prior for each media channel.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/meridian/data/load.py:1094\u001b[39m, in \u001b[36mCsvDataLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> input_data.InputData:\n\u001b[32m   1092\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Reads data from a CSV file and returns an `InputData` object.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_df_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/meridian/data/load.py:832\u001b[39m, in \u001b[36mDataFrameDataLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> input_data.InputData:\n\u001b[32m    828\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Reads data from a dataframe and returns an InputData object.\"\"\"\u001b[39;00m\n\u001b[32m    830\u001b[39m   builder = \u001b[43mdata_frame_input_data_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrameInputDataBuilder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkpi_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkpi_type\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_kpi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcoord_to_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcoord_to_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcoord_to_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgeo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    838\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.coord_to_columns.population \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.df.columns:\n\u001b[32m    839\u001b[39m     builder.with_population(\n\u001b[32m    840\u001b[39m         \u001b[38;5;28mself\u001b[39m.df, \u001b[38;5;28mself\u001b[39m.coord_to_columns.population, \u001b[38;5;28mself\u001b[39m.coord_to_columns.geo\n\u001b[32m    841\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/meridian/data/data_frame_input_data_builder.py:61\u001b[39m, in \u001b[36mDataFrameInputDataBuilder.with_kpi\u001b[39m\u001b[34m(self, df, kpi_col, time_col, geo_col)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m### Validate ###\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_cols(kpi_df, [kpi_col, time_col], [geo_col])\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkpi_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeo_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m### Transform ###\u001b[39;00m\n\u001b[32m     64\u001b[39m data = kpi_df.set_index([geo_col, time_col])[kpi_col].dropna()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/meridian/data/data_frame_input_data_builder.py:586\u001b[39m, in \u001b[36mDataFrameInputDataBuilder._validate_coords\u001b[39m\u001b[34m(self, df, geo_col, time_col)\u001b[39m\n\u001b[32m    581\u001b[39m df_grouped = df.sort_values(time_col).groupby(geo_col)[time_col]\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(df_grouped.count() != df_grouped.nunique()):\n\u001b[32m    583\u001b[39m   \u001b[38;5;66;03m# Currently we raise errors for all duplicate geo time entries. Might\u001b[39;00m\n\u001b[32m    584\u001b[39m   \u001b[38;5;66;03m# want to consider silently dropping dupes for column values that are\u001b[39;00m\n\u001b[32m    585\u001b[39m   \u001b[38;5;66;03m# the same (e.g. {geo: ['a', 'a'], 'time': ['1', '1'], kpi: [120, 120]})\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDuplicate entries found in the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m\u001b[33m column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    588\u001b[39m times_by_geo = df_grouped.apply(\u001b[38;5;28mlist\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(t != times_by_geo[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m times_by_geo[\u001b[32m1\u001b[39m:]):\n",
      "\u001b[31mValueError\u001b[39m: Duplicate entries found in the 'time' column."
     ]
    }
   ],
   "source": [
    "coord_to_columns = load.CoordToColumns(\n",
    "    time='date_id',\n",
    "    #geo='reach_country_id',\n",
    "    controls=[],\n",
    "    #population='population',\n",
    "    kpi='clicks',\n",
    "    #revenue_per_kpi='revenue_per_conversion',\n",
    "    media=[\n",
    "        'facebook_impressions',\n",
    "        'linkedin_impressions',\n",
    "        'google_impressions',\n",
    "        'bing_impressions',\n",
    "        'tiktok_impressions',\n",
    "        'twitter_impressions',\n",
    "        'reddit_impressions',\n",
    "        'instagram_impressions',\n",
    "    ],\n",
    "    media_spend=[\n",
    "        'facebook_spend',\n",
    "        'linkedin_spend',\n",
    "        'google_spend',\n",
    "        'bing_spend',\n",
    "        'tiktok_spend',\n",
    "        'twitter_spend',\n",
    "        'reddit_spend',\n",
    "        'instagram_spend',\n",
    "    ],\n",
    "    #organic_media=['Organic_channel0_impression'],\n",
    "    #non_media_treatments=['Promo'],\n",
    ")\n",
    "\n",
    "correct_media_to_channel = {\n",
    "    'facebook_impressions': 'Facebook',\n",
    "    'linkedin_impressions': 'LinkedIn',\n",
    "    'google_impressions': 'Google',\n",
    "    'bing_impressions': 'Bing',\n",
    "    'tiktok_impressions': 'TickTock',\n",
    "    'twitter_impressions': 'Twitter',\n",
    "    'reddit_impressions': 'Reddit',\n",
    "    'instagram_impressions': 'Instagram',\n",
    "}\n",
    "correct_media_spend_to_channel = {\n",
    "    'facebook_spend': 'Facebook',\n",
    "    'linkedin_spend': 'LinkedIn',\n",
    "    'google_spend': 'Google',\n",
    "    'bing_spend': 'Bing',\n",
    "    'tiktok_spend': 'TickTock',\n",
    "    'twitter_spend': 'Twitter',\n",
    "    'reddit_spend': 'Reddit',\n",
    "    'instagram_spend': 'Instagram',\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "loader = load.CsvDataLoader(\n",
    "    csv_path=\"/Users/aaronmeagher/Work/google_meridian/google/ws_attempt/data/raw_data/regularized_normalised_df_2.csv\",\n",
    "    kpi_type='non_revenue',\n",
    "    coord_to_columns=coord_to_columns,\n",
    "    media_to_channel=correct_media_to_channel,\n",
    "    media_spend_to_channel=correct_media_spend_to_channel,\n",
    ")\n",
    "\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "roi_mu = 1 #0.2     # Mu for ROI prior for each media channel.\n",
    "roi_sigma = 5 #0.9  # Sigma for ROI prior for each media channel.\n",
    "prior = prior_distribution.PriorDistribution(\n",
    "    roi_m=tfp.distributions.LogNormal(roi_mu, roi_sigma, name=constants.ROI_M)\n",
    ")\n",
    "model_spec = spec.ModelSpec(prior=prior, media_effects_dist='normal')\n",
    "\n",
    "mmm = model.Meridian(input_data=data, model_spec=model_spec)\n",
    "\n",
    "mmm.sample_prior(500)\n",
    "mmm.sample_posterior(n_chains=7, n_adapt=500, n_burnin=500, n_keep=2000, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bac48171",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mmm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmmm\u001b[49m.geos)\n",
      "\u001b[31mNameError\u001b[39m: name 'mmm' is not defined"
     ]
    }
   ],
   "source": [
    "print(mmm.geos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c343572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
